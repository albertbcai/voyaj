# Eval Framework Roadmap

Detailed plan for future evaluation framework enhancements.

---

## ‚úÖ Phase 1: State Transition Testing (COMPLETE)

**Status:** ‚úÖ Shipped

**What we built:**
- Scenario runner for multi-step conversations
- 7 initial test scenarios (JSON definitions)
- Full conversation logging with timestamps
- Visual UI to view conversations like group chat
- Metrics and reporting (pass/fail rates)
- Comprehensive documentation

**Value delivered:**
- Fast feedback loop (2-5 min vs hours)
- Easy iteration (edit JSON, see results)
- Catches state transition bugs
- Visual debugging in UI

**Time invested:** ~6 hours

---

## üìã Phase 2: Agent Isolation Tests

**Status:** üî≤ Not started

**Goal:** Test each agent's AI decisions in isolation to catch classification bugs before they cascade into state issues

### What to Build

#### 2.1 Agent Test Harness
```
eval/agents/
‚îú‚îÄ‚îÄ coordinator.eval.js      # Test coordinator AI calls
‚îú‚îÄ‚îÄ voting.eval.js           # Test voting agent classification
‚îú‚îÄ‚îÄ parser.eval.js           # Test parser accuracy
‚îú‚îÄ‚îÄ responder.eval.js        # Test responder quality
‚îî‚îÄ‚îÄ common/
    ‚îú‚îÄ‚îÄ assertions.js        # Custom assertions
    ‚îî‚îÄ‚îÄ fixtures.js          # Shared test data
```

#### 2.2 Test Coverage by Agent

**Coordinator Agent** (`eval/agents/coordinator.eval.js`)
- ‚úÖ Name validation: "Sarah" ‚Üí is name, "March" ‚Üí not name
- ‚úÖ Question routing: "What dates work?" ‚Üí question intent
- ‚úÖ Date vs destination detection: "March" ‚Üí date, "Paris" ‚Üí destination
- ‚úÖ Organizing language detection: "Let's make a spreadsheet" ‚Üí proactive control

**Voting Agent** (`eval/agents/voting.eval.js`)
- ‚úÖ Destination extraction: "Tokyo and Paris" ‚Üí ["Tokyo", "Paris"]
- ‚úÖ Vote parsing: "1" ‚Üí option 1, "Tokyo" ‚Üí option 1 (if Tokyo is option 1)
- ‚úÖ Vote with enthusiasm: "1 - Tokyo!!!" ‚Üí option 1
- ‚úÖ Non-vote detection: "This doesn't look right" ‚Üí null
- ‚úÖ Vague preference: "somewhere with good food" ‚Üí vague (not destination)

**Parser Agent** (`eval/agents/parser.eval.js`)
- ‚úÖ Date parsing accuracy: "March 15-22" ‚Üí {start: "2025-03-15", end: "2025-03-22"}
- ‚úÖ Various date formats: "April 1 to 10", "March", "flexible in April"
- ‚úÖ Date validation: end > start, future dates, within 2 years
- ‚úÖ Flight parsing: "BOOKED United 154" ‚Üí {airline: "United", flight: "154"}

**Responder Agent** (`eval/agents/responder.eval.js`)
- ‚úÖ Response decision: Should respond vs skip (based on context)
- ‚úÖ Tone selection: Control vs helper tone
- ‚úÖ Content quality: Includes destination, pending members, next steps

#### 2.3 Accuracy Metrics

**Output format:**
```bash
npm run eval:agents

Intent Detection: 95% accuracy (38/40 correct)
  ‚úÖ member_join: 100% (10/10)
  ‚úÖ destination_suggestion: 90% (9/10)
  ‚ùå "march" ‚Üí classified as date_availability (expected: member_join)
  ‚úÖ vote: 100% (10/10)
  ‚úÖ question: 90% (9/10)

Vote Parsing: 97% accuracy (29/30 correct)
  ‚úÖ Numeric votes: 100% (10/10)
  ‚úÖ Name votes: 90% (9/10)
  ‚úÖ Enthusiasm votes: 100% (10/10)
  ‚ùå "This doesn't look right" ‚Üí parsed as vote (expected: null)

Date Parsing: 90% accuracy (27/30 correct)
  ‚úÖ Exact dates: 95% (19/20)
  ‚úÖ Flexible: 100% (5/5)
  ‚ùå "April" ‚Üí failed to parse (expected: flexible in April)
  ‚ùå "next week" ‚Üí failed to parse
```

#### 2.4 Implementation Approach

**Test data generation:**
```javascript
// Generate variations programmatically
const memberJoinCases = [
  { input: 'Sarah', expected: 'member_join' },
  { input: 'Mike', expected: 'member_join' },
  { input: 'march', expected: 'member_join' }, // Edge case: month name
  { input: 'may', expected: 'member_join' },   // Edge case: month name
  { input: 'I\'m Sarah', expected: 'member_join' },
  { input: 'My name is Sarah', expected: 'member_join' },
  // ... 20-30 more cases
];

// Test each case
for (const { input, expected } of memberJoinCases) {
  const result = await detectIntent(trip, { body: input });
  assert.strictEqual(result.intent, expected);
}
```

**Custom assertions:**
```javascript
// eval/agents/common/assertions.js

// Exact match for discrete outputs
export function assertIntentEquals(actual, expected) {
  assert.strictEqual(actual, expected);
}

// Fuzzy match for dates (allow 1 day difference)
export function assertDateEquals(actual, expected) {
  const diff = Math.abs(new Date(actual) - new Date(expected));
  assert(diff < 24 * 60 * 60 * 1000, 'Dates should match within 1 day');
}

// Array equality (order-independent)
export function assertArrayEquals(actual, expected) {
  assert.deepStrictEqual([...actual].sort(), [...expected].sort());
}
```

### Timeline

**Estimated time:** 4-6 hours

**Breakdown:**
- Agent test harness setup: 1 hour
- Write test cases (20-30 per agent): 2-3 hours
- Metrics and reporting: 1 hour
- Documentation: 30 min

### Success Criteria

- ‚úÖ 95%+ accuracy on all agent classifications
- ‚úÖ Tests run in under 2 minutes
- ‚úÖ Clear failure reports ("Intent detection: 92% ‚Üí Need to improve")
- ‚úÖ Easy to add new test cases

---

## üìù Phase 3: Response Quality Evaluation

**Status:** üî≤ Not started

**Goal:** Grade all bot responses for factual correctness, completeness, and consistency

### What to Build

#### 3.1 Response Evaluator
```
eval/responses/
‚îú‚îÄ‚îÄ evaluator.js             # Grade individual responses
‚îú‚îÄ‚îÄ metrics.js               # Aggregate metrics
‚îî‚îÄ‚îÄ fixtures/
    ‚îú‚îÄ‚îÄ good-responses.json  # Examples of good responses
    ‚îî‚îÄ‚îÄ bad-responses.json   # Examples of bad responses
```

#### 3.2 Evaluation Criteria

**Factual Correctness (Must-haves)**
- ‚úÖ Mentions correct destination when relevant
- ‚úÖ Mentions pending member names (who hasn't acted yet)
- ‚úÖ Accurate counts (e.g., "2/3 members have voted")
- ‚úÖ Correct stage information
- ‚ùå No hallucinations (made-up destinations, fake member names)

**Completeness (Should-haves)**
- ‚úÖ Includes next step guidance ("Now share your dates")
- ‚úÖ Acknowledges user action ("Great! Tokyo is added")
- ‚úÖ Provides context (why we're asking for something)

**Tone Consistency**
- ‚úÖ Warm and friendly
- ‚úÖ Not too verbose (2-4 sentences for control, 1-2 for helper)
- ‚úÖ Not robotic or terse

#### 3.3 Evaluation Methods

**Method 1: Checklist Validation** (fast, deterministic)
```javascript
function evaluateResponse(response, context) {
  const checks = {
    factual: {
      mentionsDestination: context.destination ?
        response.includes(context.destination) : true,
      mentionsPendingMembers: context.pendingMembers.every(name =>
        response.includes(name)),
      noHallucinations: !hasUnknownNames(response, context.allMembers),
    },
    completeness: {
      includesNextStep: hasNextStepGuidance(response),
      acknowledgesAction: hasAcknowledgment(response),
    },
    tone: {
      isWarm: !isTerse(response) && !isRobotic(response),
      isAppropriateLength: response.split('.').length >= 2 &&
                           response.split('.').length <= 4,
    }
  };

  return checks;
}
```

**Method 2: Semantic Similarity** (for fuzzy matching)
```javascript
// Compare generated response to expected response
const expected = "Great! Tokyo is on the list. Waiting for Mike and Alex.";
const actual = "Awesome! Tokyo added. Still need Mike and Alex to suggest.";

const similarity = await semanticSimilarity(expected, actual);
assert(similarity > 0.85, 'Response should be semantically similar');
```

#### 3.4 Output Format

```bash
npm run eval:responses

Evaluated 50 bot responses across 10 scenarios:

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

Factual Correctness: 94% (47/50)
  ‚úÖ Mentioned correct destination: 48/50 (96%)
  ‚úÖ Mentioned pending members: 45/50 (90%)
  ‚úÖ Accurate counts: 47/50 (94%)
  ‚ùå 3 responses missing pending member names

Completeness: 88% (44/50)
  ‚úÖ Included next step: 44/50 (88%)
  ‚úÖ Acknowledged action: 50/50 (100%)
  ‚ùå 6 responses didn't guide user on what to do next

Tone Consistency: 96% (48/50)
  ‚úÖ Warm and friendly: 50/50 (100%)
  ‚úÖ Appropriate length: 48/50 (96%)
  ‚ùå 2 responses were too terse

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

Top Issues:
  1. Missing pending member names (3 occurrences)
     Examples:
     - Step 4 in happy-path-3-members
     - Step 7 in vote-with-enthusiasm

  2. No guidance on next step (6 occurrences)
     Examples:
     - Step 5 in destination-vote-tie
     - Step 3 in flexible-dates

Overall Score: 93% (Good)
Target: 95% (Needs minor improvements)
```

### Timeline

**Estimated time:** 4-5 hours

**Breakdown:**
- Response evaluator implementation: 2 hours
- Checklist validation logic: 1 hour
- Metrics and reporting: 1 hour
- Integration with existing scenarios: 30 min
- Documentation: 30 min

### Success Criteria

- ‚úÖ 95%+ factual correctness
- ‚úÖ Clear feedback on what's missing
- ‚úÖ Runs automatically with `npm run eval`
- ‚úÖ Easy to see patterns (e.g., "often missing pending members")

---

## üß† Phase 4: Advanced Features (Future)

**Status:** üî≤ Not started

**Goal:** LLM-powered test generation, semantic matching, historical tracking

### 4.1 LLM-Generated Test Cases

**Problem:** Writing 100s of test cases by hand is tedious

**Solution:** Use Claude to generate variations
```javascript
// eval/generators/llm-generator.js

async function generateIntentTestCases(intent, count = 50) {
  const prompt = `Generate ${count} different ways a user might express a "${intent}" intent.

Intent: ${intent}

Examples for reference:
${getExampleMessages(intent)}

Return as JSON array of strings.`;

  const variations = await callClaude(prompt);
  return JSON.parse(variations);
}

// Usage
const memberJoinVariations = await generateIntentTestCases('member_join', 50);
// ‚Üí ["Sarah", "I'm Sarah", "My name is Sarah", "Sarah here", ...]
```

**What to generate:**
- ‚úÖ Member join variations (50+)
- ‚úÖ Destination suggestions (100+)
- ‚úÖ Vote variations (50+)
- ‚úÖ Date formats (50+)
- ‚úÖ Question phrasings (50+)

### 4.2 Semantic Similarity Scoring

**Problem:** Exact string match too brittle for AI-generated text

**Solution:** Use embeddings or LLM to compare meaning
```javascript
async function semanticSimilarity(text1, text2) {
  const prompt = `Rate the semantic similarity of these two texts on a scale of 0.0 to 1.0:

Text 1: "${text1}"
Text 2: "${text2}"

Return only a number between 0.0 and 1.0.`;

  const score = parseFloat(await callClaude(prompt, {
    temperature: 0.0,
    maxTokens: 10
  }));

  return score;
}

// Usage
const expected = "Great! Tokyo is on the list. Waiting for Mike and Alex.";
const actual = "Awesome! Tokyo added. Still need Mike and Alex.";

const similarity = await semanticSimilarity(expected, actual);
// ‚Üí 0.92 (very similar)

assert(similarity > 0.85, 'Responses should be semantically similar');
```

### 4.3 Historical Tracking

**Problem:** Want to see if accuracy improves or regresses over time

**Solution:** Track metrics across eval runs
```javascript
// eval/history/tracker.js

async function recordEvalRun(results) {
  const record = {
    timestamp: new Date(),
    commit: getCurrentGitCommit(),
    results: {
      scenarios: { passed: 6, failed: 1, total: 7 },
      agents: {
        intentDetection: { accuracy: 0.95 },
        voteParsing: { accuracy: 0.97 },
        dateParsing: { accuracy: 0.90 },
      },
      responses: {
        factual: 0.94,
        completeness: 0.88,
        tone: 0.96,
      }
    }
  };

  await saveToFile('eval/history/runs.jsonl', record);
}
```

**Output:**
```bash
npm run eval:history

Eval Accuracy Over Time:

Intent Detection:
  2025-01-10: 92%
  2025-01-15: 95% ‚Üë (+3%)
  2025-01-20: 93% ‚Üì (-2%) ‚ö†Ô∏è REGRESSION

Vote Parsing:
  2025-01-10: 95%
  2025-01-15: 97% ‚Üë (+2%)
  2025-01-20: 97% ‚Üí (no change)

Overall Trend: Improving (92% ‚Üí 95%)
```

### 4.4 Cost/Performance Benchmarks

**Track:**
- ‚úÖ API cost per scenario
- ‚úÖ API cost per trip flow
- ‚úÖ Latency (p50, p95, p99)
- ‚úÖ Token usage

**Output:**
```bash
npm run eval:performance

Cost Analysis:

Per Scenario (average):
  API calls: 15
  Total tokens: 2,500
  Cost: $0.015

Per Full Trip (estimate):
  Messages: 25
  API calls: 45
  Total tokens: 7,500
  Cost: $0.045

Latency:
  p50: 1.2s
  p95: 3.5s
  p99: 5.2s

Recommendations:
  ‚ö†Ô∏è Consider caching for intent detection (30% of API calls)
  ‚úÖ Current latency is acceptable (<5s p99)
```

### Timeline

**Estimated time:** 6-8 hours

**Breakdown:**
- LLM test generation: 2 hours
- Semantic similarity: 1 hour
- Historical tracking: 2 hours
- Performance benchmarks: 2 hours
- Documentation: 1 hour

### Success Criteria

- ‚úÖ Can generate 100s of test cases automatically
- ‚úÖ Semantic similarity reduces false negatives
- ‚úÖ Historical tracking shows trends
- ‚úÖ Cost/performance insights actionable

---

## üìä Summary: What to Build Next

### Immediate Priority (Do This First)

**Phase 2: Agent Isolation Tests**
- **Why:** Catch bugs at the source (agent level) before they cascade
- **Time:** 4-6 hours
- **Value:** 30% faster debugging, 20% fewer integration test failures

### Medium Priority (Do When Response Quality Matters)

**Phase 3: Response Quality Evaluation**
- **Why:** Ensure bot responses are helpful and correct
- **Time:** 4-5 hours
- **Value:** Catch bad responses before users see them

### Low Priority (Nice to Have)

**Phase 4: Advanced Features**
- **Why:** Convenience and deeper insights
- **Time:** 6-8 hours
- **Value:** Easier test creation, better trend analysis

---

## üéØ Recommended Next Steps

1. **Use Phase 1 for a few weeks** - Get familiar, add more scenarios as you find bugs
2. **Identify pain points** - Which agent breaks most often?
3. **Build Phase 2 for that agent** - Start with isolation tests for the problematic agent
4. **Expand gradually** - Add more agents as needed
5. **Phase 3 when you have real users** - Response quality matters more with production traffic

---

## ü§î Open Questions

Things to decide later:

1. **Should we test with real AI calls or mocks?**
   - Real: More accurate, slower, costs money
   - Mocks: Faster, free, but may not catch AI issues
   - Hybrid: Use real for critical paths, mocks for everything else?

2. **How to handle flaky tests?**
   - Retry logic (run 3 times, pass if 2/3 succeed)?
   - Consensus testing (run 5 times, expect 80% agreement)?
   - Tighter prompts to reduce variance?

3. **Integration with CI/CD?**
   - Run evals on every commit?
   - Only on main branch?
   - Manual trigger only?

4. **Test data management?**
   - Keep growing `definitions/` folder?
   - Organize into subdirectories?
   - Generate dynamically with LLMs?

---

**Last updated:** 2025-01-13

**Next review:** After Phase 2 completion
